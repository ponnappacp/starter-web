Answer: HDFS is a distributed file system designed to store and process large volumes of 
data across a cluster of commodity hardware. It is a core component of the Hadoop ecosystem. 
HDFS is used for its scalability, fault tolerance, and ability to handle big data workloads. 
It enables reliable storage and efficient processing of massive datasets by distributing data 
across multiple nodes in the cluster.
aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb
cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc

dddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddd